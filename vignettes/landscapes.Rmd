---
title: "Stability landscapes for weakly non-gradient flows"
author: "Pablo Rodríguez-Sánchez, Marten Scheffer & Egbert van Nes"
abstract: "Stability landscapes, usually identified with the physical concept of scalar potential, are useful tools for understanding dynamical systems. Unfortunately, the conditions for those potentials to be well defined (and thus, meaningful) are quite restrictive for higher dimensional systems. Here we present a numerical method for decomposing dynamical systems of any size in two terms, one that has an associated potential (the gradient term), and another one that lacks it (the curl term). If the magnitude of the curl part is locally small compared to the gradient part, we can still make approximate use of the concept of stability landscape. The curl to gradient ratio can be used as a local measure of the error introduced by our approximation."
date: "`r Sys.Date()`"
output:
  pdf_document:
    keep_tex: yes
    toc: false
    citation_package: natbib
  html_document:
    theme: united
    smart: true
    fig_width: 4
    fig_height: 4
    fig_caption: true
documentclass: article
classoption: onecolumn, a4paper
fontsize: 10pt
bibliography: landscapes.bib
nocite: |
  @escher
params:
  plotly_update: false
header-includes:
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usetikzlibrary{shapes,arrows}
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```

# Introduction
With knowledge becoming progressively more interdisciplinary, the relevance of science communication is increasing fast. Mathematical concepts are among the hardest topics to communicate to non-expert audiences, policy makers, and even to professionals from the so-called _soft sciences_. Visual methods are known to be, when applicable, successful ways of teaching and communicating mathematical results to non-specialists.

## Stability landscape figures
[comment1]: <> (Proved to be succesful)
One particularly successful visualization method is that of the stability landscape, also known as the rolling marble or ball-in-a-cup diagram (@Beisner2003). The idea is deeply related to the physical concept of scalar potential (@Zhou) and the mathematical of Lyapunov function (@Strogatz1994). Stability landscapes have proven to be a successful tool to understand and explain certain non-trivial concepts about dynamical systems, such as those of multistability, basin of attraction and even bifurcation, in a remarkably intuitive way (see @Beisner2003, @Scheffer2001 and figure $\ref{fig:aew}$).

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\columnwidth]{img/sl.jpg}
	\end{center}
	\caption{Example of a stability landscape from @Scheffer2001. The upper side of the figure shows the stability landscape of a one dimensional system for different values of a bifurcation parameter. The lower side shows the bifurcation diagram. This diagram proved to be a succesful tool for explaining an advanced concept in bifurcation theory such as the Fold bifurcation.}
	\label{fig:aew}
\end{figure}

[comment2]: <> (Why does it work?)
The main reason for the success of this picture comes from the fact that stability landscapes are built as an analogy with our most familiar dynamical system: movement. Particularly, the movement of a marble along a curved surface under the influence of its own weight and a strong frictional force. It is important to stress the fact that under this picture there's not such a thing as inertia. The accurate analogy is that of a marble rolling in a surface while submerged inside a very viscous fluid (@Strogatz1994, @Pawlowski2006). This explains why our intuition, based in what we know about movement in our everyday life, works so well here.

In stability landscapes the position of the marble represents the state of the system at a given time. With this picture in mind, the shape of the surface summarizes the underlying dynamical equations, being the slope the driver of the movement. Under this picture, the peaks on the undulated surface represent unstable equilibrium states and the wells represent stable equilibrium states. Different basins of attraction are separated by _mountain ridges_ in the surface. Summarizing: the marble naturally rolls downhill to the lowest point of its basin of attraction.

[comment3]: <> (Limitations)
Like with any other analogy, it is important to be aware of its limitations. The most important one is the fact that, for flows of more than one dimension, such a surface doesn't exist in general. To get an intutive feeling of why this is true, picture a flow with a stable cyclic attractor. We need a surface where our marble can rotate in a closed loop while always going downhill. The kind of surface that only exists on M.C. Escher's surrealistic paintings (see figure $\ref{fig:escher}$).

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\columnwidth]{img/escher.jpg}
	\end{center}
	\caption{M.C. Escher's \textit{Klimmen en dalen}. Lithography, 1960.}
	\label{fig:escher}
\end{figure}

Another intuitive approach is the following: if we want to summarize, for instance, a two dimensional flow in a single surface, the height at each point of such a surface should contain the information about the derivatives of both states. That is, one number, height, codes two numbers. The process of computing the surface is a process of information compression, and it is known from mathematics that only gradient fields allow this kind of compression. In one dimensional systems there is no compression of information (one number, height, corresponds to one number, the derivative of the only state), and this is one of the reasons stability landscapes of one dimensional systems can always be computed.

## Mathematical background

From a mathematical point of view, the relationship between the surface $V(x,y)$ and the dynamics is given by equation $\ref{eq:2D}$.

\begin{equation}
\label{eq:2D}
  \begin{cases}
    \frac{dx}{dt} = f(x,y) = - \frac{\partial V}{\partial x} \\
    \frac{dy}{dt} = g(x,y) = - \frac{\partial V}{\partial y}
  \end{cases}
\end{equation}

Such a surface $V(x,y)$ exists (and thus can be computed) if and only if the crossed derivatives of the elements of the flow are equal (equation $\ref{eq:2Dcond}$).

\begin{equation}
\label{eq:2Dcond}
\frac{\partial f}{\partial y} = \frac{\partial g}{\partial x}
\end{equation}

Flows satisfying equation $\ref{eq:2Dcond}$ are known as conservative, irrotational or gradient flows (cf. section 8.3 of @Marsden2003). It is important to note that equation $\ref{eq:2Dcond}$ is an equality of functions, meaning that both of them are point-to-point identical. If an only if condition $\ref{eq:2Dcond}$ holds, we can use a line integral (@Marsden2003, section 7.2) to invert $\ref{eq:2D}$ and thus calculate $V(x,y)$ using the flow equations $f(x,y)$ and $g(x,y)$ as an input (equation $\ref{eq:2Dint}$).

\begin{equation}
\label{eq:2Dint}
V(x,y) = V(x_0, y_0) - \int^{x}_{x_0} f(\xi, y_0) d\xi - \int^{y}_{y_0} g(x, \eta) d\eta
\end{equation}

## Generalization

Dynamics in equation $\ref{eq:2D}$ and the condition for the crossed derivatives $\ref{eq:2Dcond}$ can be straightforwardly generalized (see equations $\ref{eq:anyD}$ and $\ref{eq:anyDcond}$) to systems with an arbitrary number of state variables $\vec x = (x_1, ..., x_n)$.

\begin{equation}
\label{eq:anyD}
  \frac{d x_i}{dt} = f_i(\vec x) = -\frac{\partial V}{\partial x_i} : i = 1..n
\end{equation}

\begin{equation}
\label{eq:anyDcond}
  \frac{\partial f_i}{\partial x_j} = \frac{\partial f_j}{\partial x_i} : i \neq j
\end{equation}

Where once again condition $\ref{eq:anyDcond}$ is a requirement for  $\ref{eq:anyD}$ to make sense. If that's the case the potential is given by equation $\ref{eq:anyDint}$, where the line integral is computed along the curve $\Gamma$ joining the points $\vec x_0$ and $\vec x$.

\begin{equation}
\label{eq:anyDint}
V(\vec x) = V(\vec x_0) - \int_\Gamma \sum_{i=1}^n f_i(\vec x) dx_i
\end{equation}

It is important to note that the number of equations $N$ contained in condition $\ref{eq:anyDcond}$ grows with the dimensionality of the system $D$, particularly following the series of triangular numbers $N = \frac{1}{2}(D-1)D$. Thus, the higher the dimensionality, the harder is to fulfill condition $\ref{eq:anyDcond}$. As a side effect, we see that in one dimensional systems the condition contains $0$ equations and is thus automatically fulfilled, meaning that one dimensional systems always have a well defined landscape.

Using vector notation and the nabla operator $\vec \nabla$ both the dynamical equation and the condition can be written compactly for any number of dimensions(see equation $\ref{eq:vec}$ and condition $\ref{eq:vecCond}$).

\begin{equation}
\label{eq:vec}
  \frac{d\vec x}{dt} = \vec f(\vec x) = - \vec{\nabla{V}}
\end{equation}

\begin{equation}
\label{eq:vecCond}
  \vec \nabla \times \vec f(\vec x) = \vec 0
\end{equation}

and, if $\ref{eq:vecCond}$ holds, the potential reads $\ref{eq:vecInt}$.

\begin{equation}
\label{eq:vecInt}
V(\vec x) = V(\vec x_0) - \int_\Gamma \vec f(\vec x) \cdot d\vec x
\end{equation}

The nabla operator is used to generalize the concept of derivative to functions with more than one argument. For more information see @Marsden2003 or any other introductory text about vector calculus. During the rest of this paper we'll focus only in the two dimensional case shown in equation $\ref{eq:2D}$. There are two reasons for this choice. The first is that two-dimensional flows are the simpler systems that can lack a stability landscape (or equivalently: one-dimensional systems always have one). The second reason is that, being interested in visualization, we want to focus on _"surfaces"_ like $y = V(x)$ (i.e.: curves) and $z = V(x,y)$ (i.e.: surfaces), and not in hypersurfaces with too many dimensions to plot like $u = V(x,y,z)$. We made this choice motivated by didactical considerations despite the algorithm we developed can handle general n-dimensional flows.

## Summary table

|           |                           1 dimension |                                                                       n dimensions |                               with matrix notation |
|:----------|--------------------------------------:|-----------------------------------------------------------------------------------:|---------------------------------------------------:|
| Flow      |                       $f(x) = -V'(x)$ |                        $f_i(\vec x) = -\frac{\partial V}{\partial x_i} : i = 1..n$ |                $\vec f(\vec x) = -\vec{\nabla{V}}$ |
| Condition |                                  none | $\frac{\partial f_i}{\partial x_j} = \frac{\partial f_j}{\partial x_i} : i \neq j$ |       $\vec \nabla \times \vec f(\vec x) = \vec 0$ |
| Potential | $V(x_0) - \int^{x}_{x_0} f(\xi) d\xi$ |                          $V(\vec x_0) - \int_\Gamma \sum_{i=1}^n f_i(\vec x) dx_i$ | $V(\vec x_0) - \int_\Gamma \vec f(\vec x) d\vec x$ |

# Methods
The method we propose is based on the numerical decomposition of our field in two components, a conservative or gradient part and a non-conservative or curl part:

\begin{equation}
\label{eq:FieldDecomposition}
\vec f (\vec x) = \vec f_{gradient} (\vec x) + \vec f_{curl} (\vec x)
\end{equation}

$\vec f_{gradient} (\vec x)$ captures all the part of the system that can be associated to a scalar potential, while $\vec f_{curl} (\vec x)$ represents the deviation from this ideal, conservative case. We'll use $\vec f_{gradient} (\vec x)$ to compute a scalar potential, which we will call pseudopotential. The absolute error of this approach will be given by $\parallel \vec f_{curl} (\vec x) \parallel$.

The reader is probably familiar with the concept of linearization, also known as Taylor expansion. Any sufficiently smooth and continuous vector field $\vec f(\vec x)$ can be approximated around the point $\vec{x_0}$ using equation $\ref{eq:TaylorExp}$, where $J(\vec{x_0})$ is the jacobian matrix evaluated at the approximation point $\vec{x_0}$ and $\Delta \vec x$ is defined as the vector distance to the approximation point, that is, $\Delta \vec x = \vec x - \vec{x_0}$.

\begin{equation}
\label{eq:TaylorExp}
\vec f(\vec x) \approx \vec f(\vec{x_0}) + J(\vec{x_0}) \cdot \Delta \vec x + \mathcal{O} \left( \| \vec \Delta x \| ^2 \right)
\end{equation}

If we compute equation $\ref{eq:TaylorExp}$ in the surroundings of $\vec{x_0}$ we can safely neglect the terms of order $2$ and higher. In that situation, the condition $\ref{eq:anyDcond}$ becomes a very simple restriction on the jacobian matrix (see equation $\ref{eq:jaccond}$): for the system to be conservative, its jacobian has to be symmetric.

\begin{equation}
\label{eq:jaccond}
J_{ij} = J_{ji}
\end{equation}

We know from basic linear algebra that any square matrix can be uniquely decomposed as the sum of a skew and a symmetric matrix (see equation $\ref{eq:SkewSymDec}$, where $t$ represents transposition).

\begin{equation}
\label{eq:SkewSymDec}
M = \frac{1}{2} \left( M + M^t \right) + \frac{1}{2} \left( M - M^t \right) \equiv M_{Symm} + M_{Skew}
\end{equation}

Using the skew symmetric decomposition described in equation \ref{eq:SkewSymDec}, we can rewrite \ref{eq:TaylorExp} as:

\begin{equation}
\label{eq:TaylorExpDecomposed}
\vec f(\vec x) = \vec f(\vec{x_0}) + J_{Symm}(\vec{x_0}) \cdot \Delta \vec x + J_{Skew}(\vec{x_0}) \cdot \Delta \vec x + \mathcal{O} \left( \| \vec \Delta x \| ^2 \right)
\end{equation}

This trick is interesting because it represents a natural decomposition of the field $\vec f(\vec x)$ in a gradient and a non-gradient term. More specifically, the only non-gradient term is the one containing the skew part of the jacobian\footnote{This is deeply related with the Helmholtz decomposition: it is proven that, for a huge range of easy to fulfil conditions, any field can be decomposed as $\vec f (\vec x) = - \vec \nabla V (\vec x) + \vec \nabla \times \vec A (\vec x )$, where $V$ and $\vec A$ are known as scalar and vector potentials. Both of them have a straightforward natural interpretation, and are widely used, for instance, in electromagnetism and fluid dynamics}.

\begin{equation}
\label{eq:TaylorExpDecomposedAndGradient}
\vec f(\vec x) = - \vec \nabla V(\vec{x}) + J_{Skew}(\vec{x_0}) \cdot \Delta \vec x + \mathcal{O} \left( \| \vec \Delta x \| ^2 \right)
\end{equation}

This means that systems like \ref{eq:TaylorExp} are only gradient when their jacobians are symmetric and, thus, the skew component is a matrix of zeros.

Additionally, $V(\vec x)$ can be computed analytically by direct integration:

\begin{equation}
\label{eq:Potential}
V(\vec{x}) = V(\vec{x_0}) - \vec f(\vec x_0) \cdot \Delta \vec x - \frac{1}{2} \Delta \vec x^t \cdot J_{Symm} (\vec{x_0}) \cdot \Delta \vec x + \mathcal{O} \left( \| \vec \Delta x \| ^3 \right)
\end{equation}

\subsection{Numerical algorithm}
\label{subsec:NumericalAlgorithm}
The previous ideas open the possibility of an easy to implement numerical method using the following steps:

\begin{enumerate}
\item Create a grid in the phase plane $\left\lbrace \vec x_i \right\rbrace$
\item Calculate the jacobian $J(\vec{x_i})$ in each grid point
\item Decompose each jacobian in its skew $J_{Skew}(\vec{x_i})$ and symmetric $J_{Symm}(\vec{x_i})$ components
\item Assign potential 0 to the first point in the grid
\item Use each $J_{Symm}$ to compute the (pseudo)potential point by point, using equation $\ref{eq:Potential}$
\item Use the values of $J_{Skew}(\vec{x_i})$ as a measure of the quality of the approximation point by point (the higher the values in the skew matrix relative to the symmetric ones, the worse)
\end{enumerate}

\subsubsection{One dimensional case}
\label{subsubsec:1Dcase}
In the one-dimensional case it's always possible to find an exact scalar potential.

\begin{equation}
\label{eq:Field1D}
f(x) = f(x_i) + J(x_i) \cdot (x - x_i) : x \in [x_i, x_{i+1})
\end{equation}

\begin{equation}
\label{eq:Potential1D}
V(x) = V(x_i) - f(x_i) \cdot (x - x_i) - \frac{1}{2} J(x_i) \cdot (x - x_i)^2 : x \in [x_i, x_{i+1})
\end{equation}

Equation \ref{eq:Potential1D} is a recursion relation, so it requires an initialization value:

\begin{equation}
\label{eq:Potential1DInit}
V(x_0) = V_0
\end{equation}

This allows to calculate the potential in any point of the grid using:

\begin{equation}
\label{eq:Potential1DGrid}
V(x_i) = V(x_0) - \sum_{n = 0}^{i-1} \left( f(x_n) \cdot (x_{n+1} - x_n) + \frac{1}{2} J(x_n) \cdot (x_{n+1} - x_n)^2 \right)
\end{equation}

If the grid has been constructed with a constant step $\Delta x$, equation \ref{eq:Potential1DGrid} can be simplified as:

\begin{equation}
\label{eq:Potential1DGridSimp}
V(x_i) = V(x_0) - \Delta x \sum_{n = 0}^{i-1} \left( f(x_n) + \frac{1}{2} J(x_n) \cdot \Delta x \right)
\end{equation}

Notice that equations \ref{eq:Potential1DGrid} and \ref{eq:Potential1DGridSimp} are just algebraic relationships allowing to compute the potential at any node of the grid. Furthermore, equation \ref{eq:Potential1D} can be used to interpolate between the nodes.

It can be useful to define the auxiliary function:
\begin{equation}
\label{eq:Increment1D}
\Delta V(x_i, x_{i+1}) = -\int_{x_i}^{x_{i+1}} f(s) \cdot ds = - f(x_i) \cdot (x_{i+1} - x_i) - \frac{1}{2} J(x_i) \cdot (x_{i+1} - x_i)^2
\end{equation}

so the recursion relation reads:
\begin{equation}
\label{eq:Potential1DGridWithIncrement}
V(x_i) = V(x_0) + \sum_{n = 0}^{i-1} \Delta V(x_n, x_{n+1})
\end{equation}

## Code package

# Results

## A synthetic example

## Competition model

# Discussion
- Power of visualization and intuition
- Limits of visualization and intuition
- Peters projection and geometrical limits
- Why did we abandon the phase plane?

# References
