---
title: "Climbing Escher's ladder, or how to compute stability landscapes for weakly non-gradient flows"
author: "Pablo Rodríguez-Sánchez, Marten Scheffer & Egbert van Nes"
abstract: "Stability landscapes, usually identified with the physical concept of scalar potential, are useful tools for understanding dynamical systems described by autonomous differential equations. Unfortunately, the conditions for those potentials to be well defined (and thus, meaningful) are quite restrictive for systems of two or more dimensions. Here we present a numerical method for decomposing dynamical systems of any size in two terms, one that has an associated potential (the gradient term), and another one that lacks it (the curl term). If the magnitude of the curl part is locally small compared to the gradient part, we can still make approximate use of the concept of stability landscape. The curl to gradient ratio can be used as a local measure of the error introduced by our approximation."
date: "`r Sys.Date()`"
output:
  pdf_document:
    keep_tex: yes
    toc: false
    citation_package: natbib
  html_document:
    theme: united
    smart: true
    fig_width: 4
    fig_height: 4
    fig_caption: true
documentclass: article
classoption: onecolumn, a4paper
fontsize: 10pt
bibliography: landscapes.bib
nocite: |
  @escher
params:
  plotly_update: false
header-includes:
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usetikzlibrary{shapes,arrows}
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```

# Introduction
With knowledge becoming progressively more interdisciplinary, the relevance of science communication is increasing fast. Mathematical concepts are among the hardest topics to communicate to non-expert audiences, policy makers, and even to professionals from the so-called _soft sciences_. Visual methods are known to be, when applicable, successful ways of introducing mathematical concepts and results to non-specialists.

## Stability landscape figures
[comment1]: <> (Proved to be succesful)
One particularly successful visualization method is that of the stability landscape, also known as the rolling marble or ball-in-a-cup diagram (@Beisner2003). The idea is deeply related to the physical concept of scalar potential (@Zhou) and the mathematical of Lyapunov function (@Strogatz1994). Stability landscapes have proven to be a successful tool to understand and explain certain non-trivial concepts about dynamical systems described by autonomous differential equations in a remarkably intuitive way. Examples of those concepts are multistability, basin of attraction or even bifurcation and hystheresis (see @Beisner2003, @Scheffer2001).

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\columnwidth]{img/sl.jpg}
	\end{center}
	\caption{Example of a stability landscape from @Scheffer2001. The upper side of the figure shows the stability landscape of a one dimensional system for different values of a bifurcation parameter. The lower side shows the bifurcation diagram. This diagram proved to be a succesful tool for explaining an advanced concept in bifurcation theory such as the Fold bifurcation.}
	\label{fig:aew}
\end{figure}

[comment2]: <> (Why does it work?)
The main reason for the success of this picture arises from the fact that stability landscapes are built as an analogy with our most familiar dynamical system: movement. Particularly, the movement of a marble along a curved surface under the influence of its own weight and a strong frictional force[^1]. This explains why our intuition, based in what we know about movement in our everyday life, works so well here. 

[^1]: It is important to stress the fact that under this picture there's not such a thing as inertia (@Beisner2003). The accurate analogy is that of a marble rolling in a surface while submerged inside a very viscous fluid (@Strogatz1994).

In stability landscapes (e.g.: figure $\ref{fig:aew}$) the position of the marble represents the state of the system at a given time. With this picture in mind, the shape of the surface summarizes the underlying dynamical equations, being the slope the driver of the movement. The peaks on the undulated surface represent unstable equilibrium states and the wells represent stable equilibria. Different basins of attraction are separated by _mountain ridges_ in the surface. Summarizing: the marble naturally rolls downhill to the lowest point of its basin of attraction.

[comment3]: <> (Limitations)
Like with any other analogy, it is important to be aware of its limitations. The most important one is the fact that, for flows of more than one dimension, such a surface doesn't exist in general. To get an intutive feeling of why this is true, picture a flow with a stable cyclic attractor. We need a surface where our marble can roll in a closed loop while always going downhill. The kind of surface that only exists on M.C. Escher's surrealistic paintings (see figure $\ref{fig:escher}$).

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\columnwidth]{img/escher.jpg}
	\end{center}
	\caption{Detail of M.C. Escher's \textit{Klimmen en dalen}. Lithography, 1960. In this surface, the walkers can walk in a closed loop that permanently goes downhill. Unfortunately, such a surface only exists in the imagination of surrealist painters.}
	\label{fig:escher}
\end{figure}

An alternative intuitive approach is the following: if we want to summarize, for instance, a two dimensional flow in a single surface, the height at each point of such a surface should contain the information about the derivatives of both states. That is, one number, height, codes two numbers. The process of computing the surface is a process of information compression, and it is known from mathematics that only gradient fields allow this kind of compression. In one dimensional systems there is no compression of information (one number, height, corresponds to one number, the derivative of the only state), and this is one of the reasons stability landscapes of one dimensional systems can always be computed.

In the present work we present a detailed overview of the conditions that a flow has to fulfill to be gradient and thus be associated with a classical potential. Next, we center our attention in flows that fail to fulfill those conditions, and we introduce an algorithm to decompose those flows as the sum of a gradient and a curl part. Each part can be used, respectivelly, to compute an associated (pseudo)potential and to measure the local error introduced by our picture. 

For those users less familiar with mathematics or programming, we provide a ready to use _R_ package that implements the algorithm described in the present paper.

## Mathematical background

From a mathematical point of view, the relationship between the surface $V(x,y)$ and the dynamics is given by equation $\ref{eq:2D}$.

\begin{equation}
\label{eq:2D}
  \begin{cases}
    \frac{dx}{dt} = f(x,y) = - \frac{\partial V}{\partial x} \\
    \frac{dy}{dt} = g(x,y) = - \frac{\partial V}{\partial y}
  \end{cases}
\end{equation}

Such a surface $V(x,y)$ exists (and thus can be computed) if and only if the crossed derivatives of the elements of the flow are equal (equation $\ref{eq:2Dcond}$).

\begin{equation}
\label{eq:2Dcond}
\frac{\partial f}{\partial y} = \frac{\partial g}{\partial x}
\end{equation}

Flows satisfying equation $\ref{eq:2Dcond}$ are known as conservative, irrotational or gradient flows (cf. section 8.3 of @Marsden2003). It is important to note that equation $\ref{eq:2Dcond}$ is an equality of functions, meaning that both of them are point-to-point identical. If an only if condition $\ref{eq:2Dcond}$ holds, we can use a line integral (@Marsden2003, section 7.2) to invert $\ref{eq:2D}$ and calculate $V(x,y)$ using the flow equations $f(x,y)$ and $g(x,y)$ as an input (equation $\ref{eq:2Dint}$).

\begin{equation}
\label{eq:2Dint}
V(x,y) = V(x_0, y_0) - \int^{x}_{x_0} f(\xi, y_0) d\xi - \int^{y}_{y_0} g(x, \eta) d\eta
\end{equation}

## Generalization

Dynamics in equation $\ref{eq:2D}$ and the condition for the crossed derivatives $\ref{eq:2Dcond}$ can be straightforwardly generalized (see equations $\ref{eq:anyD}$ and $\ref{eq:anyDcond}$) to systems with an arbitrary number of state variables $\vec x = (x_1, ..., x_n)$. Particularly, if and only if our flow equations $\frac{dx_i}{dt} = f_i(\vec x)$ satisfy the condition:

\begin{equation}
\label{eq:anyDcond}
  \frac{\partial f_i}{\partial x_j} = \frac{\partial f_j}{\partial x_i} : i \neq j
\end{equation}

then exists a surface $V(\vec x)$ verifying:

\begin{equation}
\label{eq:anyD}
  f_i(\vec x) = -\frac{\partial V}{\partial x_i} : i = 1..n
\end{equation}

and such a surface can be computed using:

\begin{equation}
\label{eq:anyDint}
V(\vec x) = V(\vec x_0) - \int_\Gamma \sum_{i=1}^n f_i(\vec x) dx_i
\end{equation}

where the line integral in $\ref{eq:anyDint}$ is computed along any[^2] curve $\Gamma$ joining the points $\vec x_0$ and $\vec x$.

[^2]: Condition $\ref{eq:anyDcond}$ guarantees that the result is independent of the shape of the chosen curve.

It is important to note that the number of equations $N$ contained in condition $\ref{eq:anyDcond}$ grows with the dimensionality of the system $D$, particularly following the series of triangular numbers $N = \frac{1}{2}(D-1)D$. Thus, the higher the dimensionality, the harder is to fulfill condition $\ref{eq:anyDcond}$. As a side effect, we see that in one dimensional systems the condition contains $0$ equations and is thus automatically fulfilled, meaning that one dimensional systems always have a well defined stability landscape.

## With vector notation

Using vector notation and the nabla operator $\vec \nabla$ both the dynamical equation and the condition can be written compactly for any number of dimensions. Particularly, if and only if our flow equations $\frac{d\vec{x}}{dt} = \vec f(\vec x)$ satisfy the condition:

\begin{equation}
\label{eq:vecCond}
  \vec \nabla \times \vec f(\vec x) = \vec 0
\end{equation}

then exists a surface $V(\vec x)$ verifying:

\begin{equation}
\label{eq:vec}
  \frac{d\vec x}{dt} = \vec f(\vec x) = - \vec{\nabla{V}}
\end{equation}

and such a surface can be computed using:

\begin{equation}
\label{eq:vecInt}
V(\vec x) = V(\vec x_0) - \int_\Gamma \vec f(\vec x) \cdot d\vec x
\end{equation}

The nabla operator is used to generalize the concept of derivative to functions with more than one argument. For more information see @Marsden2003 or any other introductory text about vector calculus. 

During the rest of this paper we'll focus only in the two dimensional case shown in equation $\ref{eq:2D}$. There are two reasons for this choice. The first is that two-dimensional flows are the simpler systems that can lack a stability landscape (or equivalently: one-dimensional systems always have one). The second reason is that, being interested in visualization, we want to focus on _"surfaces"_ like $y = V(x)$ (i.e.: curves) and $z = V(x,y)$ (i.e.: surfaces), and not in hypersurfaces with too many dimensions to plot like $u = V(x,y,z)$. We made this choice motivated by didactical considerations despite the algorithm we developed can handle general n-dimensional flows.

# Methods
The method we propose is based on the decomposition of our field in a conservative or gradient part and a non-conservative or curl part (see equation $\ref{eq:FieldDecomposition}$).

\begin{equation}
\label{eq:FieldDecomposition}
\vec f (\vec x) = \vec f_{grad} (\vec x) + \vec f_{curl} (\vec x)
\end{equation}

$\vec f_{grad} (\vec x)$ captures all the part of the system that can be associated to a scalar potential, while $\vec f_{curl} (\vec x)$ represents the deviation from this ideal, conservative case. We'll use $\vec f_{gradient} (\vec x)$ to compute a scalar potential, which we will call pseudopotential. The absolute error of this approach will be given by $\parallel \vec f_{curl} (\vec x) \parallel$. The next steps show an easy way of achieve such a decomposition.

The reader is probably familiar with the concept of linearization, also known as Taylor expansion. Any sufficiently smooth and continuous vector field $\vec f(\vec x)$ can be approximated around the point $\vec{x_0}$ using equation $\ref{eq:TaylorExp}$, where $J(\vec{x_0})$ is the jacobian matrix evaluated at the approximation point $\vec{x_0}$ and $\Delta \vec x$ is defined as the vector distance to the approximation point, that is, $\Delta \vec x = \vec x - \vec{x_0}$.

\begin{equation}
\label{eq:TaylorExp}
\vec f(\vec x) \approx \vec f(\vec{x_0}) + J(\vec{x_0}) \cdot \Delta \vec x
\end{equation}

As usual in linearization, we have neglected the terms of order $2$ and higher in equation $\ref{eq:TaylorExp}$. This approximation is valid in the surroundings of $\vec{x_0}$. For an equation like $\ref{eq:TaylorExp}$, the condition $\ref{eq:anyDcond}$ becomes a very simple restriction on the jacobian matrix (see equation $\ref{eq:jaccond}$): for the system to be gradient, its jacobian has to be symmetric.

\begin{equation}
\label{eq:jaccond}
J_{ij} = J_{ji}
\end{equation}

We know from basic linear algebra that any square matrix $M$ can be uniquely decomposed as the sum of a skew and a symmetric matrix (see equation $\ref{eq:SkewSymDec}$, where $t$ represents transposition).

\begin{equation}
\label{eq:SkewSymDec}
  \begin{cases}
    M_{Symm} = \frac{1}{2} \left( M + M^t \right) \\
    M_{Skew} = \frac{1}{2} \left( M - M^t \right)
  \end{cases}
\end{equation}

Using the skew symmetric decomposition described in equation \ref{eq:SkewSymDec}, we can rewrite \ref{eq:TaylorExp} as:

\begin{equation}
\label{eq:TaylorExpDecomposed}
\vec f(\vec x) \approx \vec f(\vec{x_0}) + J_{Symm}(\vec{x_0}) \cdot \Delta \vec x + J_{Skew}(\vec{x_0}) \cdot \Delta \vec x
\end{equation}

As we have seen from condition $\ref{eq:jaccond}$, the only term that is not gradient is the one containing $J_{Skew}$. Equation $\ref{eq:TaylorExpDecomposed}$ represents a natural, well-defined and operational way of writing our field $\vec f(\vec x)$ decomposed as in equation $\ref{eq:FieldDecomposition}$, that is, as the sum of a gradient and a non-gradient term[^3] (see $\ref{eq:decomposition}$).

[^3]: This decomposition is related with the Helmholtz decomposition: it is proven that, for a huge range of easy to fulfil conditions, any field can be decomposed as $\vec f (\vec x) = - \vec \nabla V (\vec x) + \vec \nabla \times \vec A (\vec x )$, where $V$ and $\vec A$ are known as scalar and vector potentials. Both of them have a straightforward natural interpretation, and are widely used, for instance, in electromagnetism and fluid dynamics.

\begin{equation}
\label{eq:decomposition}
  \begin{cases}
    \vec f_{grad} (\vec x) \approx \vec f(\vec{x_0}) + J_{Symm}(\vec{x_0}) \cdot \Delta \vec x \\
    \vec f_{curl} (\vec x) \approx J_{Skew}(\vec{x_0}) \cdot \Delta \vec x
  \end{cases}
\end{equation}

The gradient term can thus be associated to a stability landscape $V(\vec x)$, whose shape can be computed analytically using a line integral (see $\ref{eq:vecInt}$). Its integrated expression is given in equation $\ref{eq:Potential}$.

\begin{equation}
\label{eq:Potential}
V(\vec{x}) \approx  V(\vec{x_0}) - \vec f(\vec x_0) \cdot \Delta \vec x - \frac{1}{2} \Delta \vec x^t \cdot J_{Symm} (\vec{x_0}) \cdot \Delta \vec x
\end{equation}

Using this potential $V(\vec x)$, we can rewrite our dynamical system as in equation $\ref{eq:MainResult}$. Due to the approximations used, this result will only be true on the vicinity of $\vec{x_0}$.

\begin{equation}
\label{eq:MainResult}
\vec f(\vec x) \approx -\vec \nabla V + \vec f_{curl}(\vec x)
\end{equation}

The stability landscape described in $\ref{eq:Potential}$ not only has been derived from a linearized system, but also completely neglects the effects of the curl part of the flow. It is, thus, a local approximation (only valid on the vicinity of $\vec{x_0}$), and as with any other approximation we need a way to estimate its error. From equation $\ref{eq:MainResult}$ it is apparent that we can use $\vec f_{curl} (\vec x)$ as an approximation of the local error introduced by our algorithm ($\ref{eq:error}$).

\begin{equation}
\label{eq:error}
err(\vec x_0) \approx \| \vec f_{curl}(\vec x_0) \| \sim \| J_{Skew}(\vec{x_0}) \|
\end{equation}

## Numerical algorithm
Equation $\ref{eq:Potential}$ can be rewritten as $\ref{eq:PotentialNum}$, where $\vec{x_0}$ represents the approximation point, $\vec{x_1}$ represents a neighbouring point and the potential difference between both points is given by $\Delta V(\vec{x_1}, \vec{x_0}) \equiv V(\vec{x_1}) - V(\vec{x_0})$.

\begin{equation}
\label{eq:PotentialNum}
\Delta V(\vec{x_1}, \vec{x_0}) \approx - \vec f(\vec x_0) \cdot (\vec{x_1} - \vec{x_0}) - \frac{1}{2} (\vec{x_1} - \vec{x_0})^t \cdot J_{Symm} (\vec{x_0}) \cdot (\vec{x_1} - \vec{x_0})
\end{equation}

Provided we know the value of the potential at one point $\vec x_{previous}$, equation $\ref{eq:PotentialNum}$ allows us to estimate the potential at a different point $\vec x_{next}$ (cf.: equation $\ref{eq:Iterator}$).

\begin{equation}
\label{eq:Iterator}
V(\vec x_{next}) \approx V(\vec x_{previous}) + \Delta V(\vec x_{next}, \vec x_{previous})
\end{equation}

The abovementioned ideas can be applied to the numerical computation of the (pseudo)potential over a grid of points by following this list of steps:

\begin{enumerate}
  \item Create a grid in the phase plane $\left\lbrace \vec x_i \right\rbrace$
  \item Calculate the jacobian $J(\vec{x_i})$ in each grid point
  \item Decompose each jacobian in its skew $J_{Skew}(\vec{x_i})$ and symmetric $J_{Symm}(\vec{x_i})$ components
  \item Assign potential 0 to the first point in the grid
  \item Use equation $\ref{eq:Iterator}$ to iterativelly calculate the values of the potential at each point of the grid
  \item Use the values of $J_{Skew}(\vec{x_i})$ as a measure of the quality of the approximation point by point.
\end{enumerate}

By introducing a grid, we expect the linearization error to decrease with square of the grid's step size. The more fundamental error due to ignoring the curl component of our field cannot be affected by the grid's step choice, but at least the algorithm provides us with a measure of its magnitude point-by-point.

## Two dimensional example

For instance, to calculate the value of $V$ at the point $(x_3, y_2)$ of a grid, we should begin by assigning $0$ to the potential at $V(x_0, y_0)$. Then, we need a trajectory that goes from $(x_0,y_0)$ to $(x_3, y_2)$, stopping in the intermediate grid points (see figure).


```{r steps, fig.width = 4, fig.height = 3, fig.align = 'center'}
library(ggplot2)
d=data.frame(x=c(0,1,2,3,3), y=c(0,0,0,0,1), vx=c(1,2,3,3,3), vy=c(0, 0, 0, 1, 2))

ggplot() + 
geom_segment(data=d, mapping=aes(x=x, y=y, xend=vx, yend=vy), arrow=arrow(), size=2, color="black") + 
geom_point(data=d, mapping=aes(x=x, y=y), size=4, shape=21, fill="white") +
geom_point(data=d, mapping=aes(x=vx, y=vy), size=4, shape=21, fill="white") +
scale_x_continuous(breaks=c(0, 1, 2, 3), labels=c("x_0", "x_1", "x_2", "x_3")) +
scale_y_continuous(breaks=c(0, 1, 2, 3), labels=c("y_0", "y_1", "y_2", "y_3")) +
theme(panel.grid.minor.x = element_blank()) +
theme(panel.grid.minor.y = element_blank())
```

In the first step we go from $(x_0, y_0)$ to $(x_1, y_0)$. The new potential is thus (using $\ref{eq:Iterator}$):

\begin{equation}
\label{eq:IteratorEx1}
V(x_1, y_0) \approx V(x_0, y_0) + \Delta V(x_1, y_0; x_0, y_0)
\end{equation}

The next two steps continue in the horizontal direction, all the way to $(x_3, y_0)$. The value of the potential there is:

\begin{equation}
\label{eq:IteratorEx2}
V(x_3, y_0) \approx V(x_0, y_0) + \Delta V(x_1, y_0; x_0, y_0) + \Delta V(x_2, y_0; x_1, y_0) + \Delta V(x_3, y_0; x_2, y_0)
\end{equation}

Now, to reach our destination $(x_3, y_2)$ we have to move in the vertical direction:

\begin{equation}
\label{eq:IteratorEx3}
\begin{split}
V(x_3, y_2) \approx V(x_0, y_0) + \Delta V(x_1, y_0; x_0, y_0) + \Delta V(x_2, y_0; x_1, y_0) + \Delta V(x_3, y_0; x_2, y_0) + \\
+ \Delta V(x_3, y_1; x_3, y_0) + \Delta V(x_3, y_2; x_3, y_1)
\end{split}
\end{equation}

Generalizing the previous example we see that, for a generic point $(x_i, y_j)$, we can compute the approximate potential using the closed formula $\ref{eq:NumericalRecipe}$.

\begin{equation}
\label{eq:NumericalRecipe}
V(x_i, y_j) = V(x_0, y_0) + \sum_{k = 1}^i \Delta V(x_k, y_0; x_{k-1}, y_0) + \sum_{l = 1}^j \Delta V(x_i, y_l; x_i, y_{l-1})
\end{equation}

The attentive reader may have noticed that formula $\ref{eq:NumericalRecipe}$ has been derived sweeping first in the horizontal direction and next in the vertical one. Of course, we can choose different paths of summation. Because we are building our potential neglecting the curl part of our flow, we know by Green's theorem that our results will converge to the same solution regardless of the chosen path.

## Code package
In the spirit of reproducible research, we published an _R_ package that applies the abovementioned algorithm. It is freely available at: TBA.

# Results

## A synthetic example
We tested our algorithm against a field of the form given in $\ref{eq:TestField}$, where $p_x$ and $p_y$ are non-gradient perturbations. When we chose those perturbations to be zero, the field becomes gradient and corresponds with a four-well potential. Our algorithm rendered it succesfully (cf. figure $\ref{fig:synthetic}$).

\begin{equation}
\label{eq:TestField}
  \begin{cases}
    f(x,y) = -x (x^2 - 1) + p_x(x,y)\\
    g(x,y) = -y (y^2 - 1) + p_y(x,y)
  \end{cases}
\end{equation}

When non-zero perturbations are introduced, the error obtained correlates with the regions of the plane $(x,y)$ were we defined the perturbations to be maximum. If the perturbations are locally weak relative to the gradient term, a four-well potential is still recognizable $\ref{fig:synthetic}$.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=1.00\columnwidth]{img/synthetic.png}
	\end{center}
	\caption{In all three figures the red and blue lines represent the nullclines of $x$ and $y$, respectivelly. Several trajectories are plotted in black. The contour plot in figure a) shows the potential calculated for the gradient case (i.e.: $p_x(x,y) = p_y(x,y) = 0$). As expected, the stable equilibria correspond to the wells. The contour plot in figure b) also shows the potential, but now a perturbation has been introduced. It can be noticed that the shape of the potential has been distorted. Indeed, the two equilibria at the upper side of the plot fall slightly outside the closest well. The equilibria at the botton, to the contrary, fit perfectly centered in their corresponding wells. Figure c) shows the estimated error of our computed potential. This figure warns us against trusting too much the potential we obtained in the upper and central region, and guarantees us that elsewhere it will work fine. Notice that the two upper equilibria lie in a region where the error is clearly non zero. The two lower ones, on the contrary, lie on a region where the error is very close to zero, so we can safely use the previously derived potential to visualize them.}
	\label{fig:synthetic}
\end{figure}

## A biological example

### A simple regulatory gene network
A bistable network model can be described by a set of equations like $\ref{eq:Waddington}$ (see @Bhattacharya2011).

\begin{equation}
  \label{eq:Waddington}
  \begin{cases}
    \frac{dx}{dt} = b_x - r_x x + \frac{a_x}{k_x + y^n} \\
    \frac{dy}{dt} = b_y - r_y y + \frac{a_y}{k_y + x^n}
  \end{cases}
\end{equation}

Despite this flow is clearly non-gradient, our algorithm correctly predicts the existance of two wells (see figure $\ref{fig:Waddington}$). When the role of $x$ and $y$ is not symmetrical in equation $\ref{eq:Waddington}$, the algorithm proposed in @Bhattacharya2011 cannot guarantee continuity simultaneously at the separatrix and at the vicinity of the equilibrium points.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=1.00\columnwidth]{img/waddington.png}
	\end{center}
	\caption{In both figures the red and blue lines represent the nullclines of $x$ and $y$, respectivelly. Several trajectories are plotted in black. The contour plot in figure a) shows the potential calculated for the simple gene regulatory network described in equation $\ref{eq:Waddington}$. The stable equilibria correspond to the wells. Figure b) shows the estimated error of our computed potential.}
	\label{fig:Waddington}
\end{figure}

# Discussion
Visualization is a helpful tool for processing large amounts of information.

The use of stability landscapes as a helping tool to understand one-dimensional dynamical systems achieved great success, specially in interdisciplinary research communities. A generalization to two-dimensional systems seemed to be a logical next step. Unfortunately, as we have seen, there are subtle reasons that make two (and higher) dimensional systems fundamentally different from the one-dimensional case. The generalization, straightforward as it may look, is actually impossible for most dynamical equations.

In this paper we presented one possible trick 

The concept of potential is a cornerstone of physics. For instance, we know that the time evolution of several physical systems is governed by a potential function. Such a function can be related with physical concepts like the potential energy, so its use goes way further than visualization. Generic stability landscapes lack this straightforward link with measurable quantities, and in most cases are just no more (and no less) than a visual aid. It may be worth reconsidering why do we prefer the idea of stability landscape over the good-old phase plane figure. It is true that the latter is slightly less intuitive than the stability landscape, but it has a very desirable property: existence under general circumstances.

\newpage

# Appendix

### One dimensional case
In the one-dimensional case it's always possible to find an exact scalar potential.

\begin{equation}
\label{eq:Field1D}
f(x) = f(x_i) + J(x_i) \cdot (x - x_i) : x \in [x_i, x_{i+1})
\end{equation}

\begin{equation}
\label{eq:Potential1D}
V(x) = V(x_i) - f(x_i) \cdot (x - x_i) - \frac{1}{2} J(x_i) \cdot (x - x_i)^2 : x \in [x_i, x_{i+1})
\end{equation}

Equation \ref{eq:Potential1D} is a recursion relation, so it requires an initialization value:

\begin{equation}
\label{eq:Potential1DInit}
V(x_0) = V_0
\end{equation}

This allows to calculate the potential in any point of the grid using:

\begin{equation}
\label{eq:Potential1DGrid}
V(x_i) = V(x_0) - \sum_{n = 1}^{i} \left( f(x_{n-1}) \cdot (x_n - x_{n-1}) + \frac{1}{2} J(x_{n-1}) \cdot (x_n - x_{n-1})^2 \right)
\end{equation}

If the grid has been constructed with a constant step $\Delta x$, equation \ref{eq:Potential1DGrid} can be simplified as:

\begin{equation}
\label{eq:Potential1DGridSimp}
V(x_i) = V(x_0) - \Delta x \sum_{n = 1}^i \left( f(x_{n-1}) + \frac{1}{2} J(x_{n-1}) \cdot \Delta x \right)
\end{equation}

Notice that equations \ref{eq:Potential1DGrid} and \ref{eq:Potential1DGridSimp} are just algebraic relationships allowing to compute the potential at any node of the grid. Furthermore, equation \ref{eq:Potential1D} can be used to interpolate between the nodes.

It can be useful to define the auxiliary function:
\begin{equation}
\label{eq:Increment1D}
\Delta V(x_i; x_{i-1}) = -\int_{x_{i-1}}^{x_i} f(s) \cdot ds \approx - f(x_{i-1}) \cdot (x_i - x_{i-1}) - \frac{1}{2} J(x_{i-1}) \cdot (x_i - x_{i-1})^2
\end{equation}

so the recursion relation reads:
\begin{equation}
\label{eq:Potential1DGridWithIncrement}
V(x_i) = V(x_0) + \sum_{k = 1}^{i} \Delta V(x_k; x_{k-1})
\end{equation}

Reading equation $\ref{eq:Increment1D}$ from a geometrical perspective, we see that our numerical method gets reduced to the trapezoidal rule for numerical integration when applied to one-dimensional systems.

# References
